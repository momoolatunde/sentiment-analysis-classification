{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgD_3FJwaES2"
   },
   "source": [
    "# **Sentiment Classification - Deep Learning - Basic Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLJhqTgS68An"
   },
   "source": [
    "# **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFnczcAc7Mav"
   },
   "source": [
    "**Install Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KSxo3JxS7Hxd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install the 'datasets' library \n",
    "!pip install datasets -q\n",
    "\n",
    "# install the 'spacy' library\n",
    "!pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# downloads the small English model (en_core_web_sm) for the Spacy library\n",
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-0Iv3Os7QDx"
   },
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y0ne7wIg7UU7"
   },
   "outputs": [],
   "source": [
    "# import the 'load_and_prepare_imdb_dataset' function from the 'imdb_data_loader'\n",
    "from imdb_data_loader import load_and_prepare_imdb_dataset\n",
    "\n",
    "# call the 'load_and_prepare_imdb_dataset' function to import the IMDB dataset\n",
    "trainData, testData = load_and_prepare_imdb_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTGBM1n97W6M"
   },
   "source": [
    "# **Dataset Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_8fuEvh7Zdq"
   },
   "source": [
    "**Data Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IxYSlnBL7dGO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the 'head()' method on the 'trainData' DataFrame to inspect the first five rows\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vVbKoC4i7ndU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the shape of the 'trainData' DataFrame\n",
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rjHEXnao7g0N"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First off let me say, If you haven't enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I love sci-fi and am willing to put up with a ...      0\n",
       "1  Worth the entertainment value of a rental, esp...      0\n",
       "2  its a totally average film with a few semi-alr...      0\n",
       "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
       "4  First off let me say, If you haven't enjoyed a...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the 'head()' method on the 'testData' DataFrame to inspect the first five rows\n",
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "53ZeXaAK7lF1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the shape of the 'testData' DataFrame\n",
    "testData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojlNeIIlKJwH"
   },
   "source": [
    "**Remove Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DXi_uGDuKKlC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of duplicated entries in the 'text' column of the 'trainData' DataFrame\n",
    "trainDataDuplicates = trainData['text'].duplicated().sum()\n",
    "trainDataDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "64jpgJelKM9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24904, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate rows from 'trainData' based on the 'text' column\n",
    "noTrainDataDuplicates = trainData.drop_duplicates(subset='text')\n",
    "noTrainDataDuplicatesShape = noTrainDataDuplicates.shape\n",
    "\n",
    "noTrainDataDuplicatesShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DPVb3LSWKewZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of duplicated entries in the 'text' column of the 'testData' DataFrame\n",
    "testDataDuplicates = testData['text'].duplicated().sum()\n",
    "testDataDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u2C7uaJ4KfXL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24801, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate rows from 'testData' based on the 'text' column\n",
    "noTestDataDuplicates = testData.drop_duplicates(subset='text')\n",
    "noTestDataDuplicatesShape = noTestDataDuplicates.shape\n",
    "\n",
    "noTestDataDuplicatesShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wwJa1nfKlqB"
   },
   "source": [
    "# **Basic Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_oN-5HqOR3KV"
   },
   "outputs": [],
   "source": [
    "# import the preprocessing function 'preprocess_basic' from 'basic_preprocessing'\n",
    "\n",
    "# Convert text to lowercase.\n",
    "# Remove HTML tags using BeautifulSoup.\n",
    "# Handle contractions.\n",
    "# Expand acronyms.\n",
    "# Tokenize the text using SpaCy.\n",
    "# Remove punctuation tokens.\n",
    "# Remove non-alphabetic characters.\n",
    "\n",
    "from basic_preprocessing import preprocess_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oRQihYRZKoEN"
   },
   "outputs": [],
   "source": [
    "# create copies of the noTrainDataDuplicates and noTestDataDuplicates DataFrames\n",
    "rawtrainData = noTrainDataDuplicates.copy()\n",
    "rawtestData = noTestDataDuplicates.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dtZ6gWLzKqTq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\research-dissertation\\codes\\basic_preprocessing.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "# apply the 'preprocess_basic' function to each text entry in the 'rawtrainData' DataFrame\n",
    "trainDataBasic = rawtrainData['text'].apply(preprocess_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b5sOD8xuKuxk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: i rented i am curious yellow from my video store because of all the controversy that surrounded it when it was first released in i also heard that at first it was seized by customs if it ever tried to enter this country therefore being a fan of films considered controversial i really had to see this for plot is centered around a young swedish drama student named lena who wants to learn everything she can about life in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states in between asking politicians and ordinary denizens of stockholm about their opinions on politics she has sex with her drama teacher classmates and married kills me about i am curious yellow is that years ago this was considered pornographic really the sex and nudity scenes are few and far between even then it has it is not shot like some cheaply made porno while my countrymen mind find it shocking in reality sex and nudity are a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford had sex scenes in his do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america i am curious yellow is a good film for anyone wanting to study the meat and potatoes no pun intended of swedish cinema but really this film does not have much of a plot\n",
      "Index 1: i am curious yellow is a risible and pretentious steaming pile it does not matter what one political views are because this film can hardly be taken seriously on any level as for the claim that frontal male nudity is an automatic that is not true i seen r rated films with male nudity granted they only offer some fleeting views but where are the r rated films with gaping vulvas and flapping labia nowhere because they do not exist the same goes for those crappy cable shows schlongs swinging in the breeze but not a clitoris in sight and those pretentious indie movies like the brown bunny in which we are treated to the site of vincent gallo throbbing johnson but not a trace of pink visible on chloe sevigny before crying or implying double standard in matters of nudity the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women there are no genitals on display when actresses appears nude and the same can not be said for a man in fact you generally will not see female genitals in an american film in anything short of porn or explicit erotica this alleged double standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women bodies\n",
      "Index 2: if only to avoid making this type of film in the future this film is interesting as an experiment but tells no cogent might feel virtuous for sitting thru it because it touches on so many important issues but it does so without any discernable motive the viewer comes away with no new perspectives unless one comes up with one while one mind wanders as it will invariably do during this pointless might better spend one tears in my eyes staring out a window at a tree growing\n"
     ]
    }
   ],
   "source": [
    "# print the first three preprocessed text entries (train data) to verify the preprocessing step\n",
    "for index, value in trainDataBasic.items():\n",
    "    print(f\"Index {index}: {value}\")\n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jni2W2NjKwwB"
   },
   "outputs": [],
   "source": [
    "# apply the 'preprocess_basic' function to each text entry in the 'rawtestData' DataFrame\n",
    "\n",
    "testDataBasic = rawtestData['text'].apply(preprocess_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IfkXIyy1Kxjh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: i love sci fi and am willing to put up with a lot sci fi movies tv are usually underfunded under appreciated and misunderstood i tried to like this i really did but it is to good tv sci fi as babylon is to star trek the original silly prosthetics cheap cardboard sets stilted dialogues cg that does not match the background and painfully one dimensional characters can not be overcome with a sci fi setting i sure there are those of you out there who think babylon is good sci fi tv it has it is not it has it is clichÃ©d and uninspiring while us viewers might like emotion and character development sci fi is a genre that does not take itself seriously cf star trek it may treat important issues yet not as a serious philosophy it has it is really difficult to care about the characters here as they are not simply foolish just missing a spark of life their actions and reactions are wooden and predictable often painful to watch the makers of earth know it has it is rubbish as they have to always say gene roddenberry earth otherwise people would not continue watching roddenberry ashes must be turning in their orbit as this dull cheap poorly edited watching it without advert breaks really brings this home trudging trabant of a show lumbers into space spoiler so kill off a main character and then bring him back as another actor jeeez dallas all over again\n",
      "Index 1: worth the entertainment value of a rental especially if you like action movies this one features the usual car chases fights with the great van damme kick style shooting battles with the shell load shotgun and even terrorist style bombs all of this is entertaining and competently handled but there is nothing that really blows you away if you have seen your share plot is made interesting by the inclusion of a rabbit which is clever but hardly profound many of the characters are heavily stereotyped the angry veterans the terrified illegal aliens the crooked cops the indifferent feds the bitchy tough lady station head the crooked politician the fat federale who looks like he was typecast as the mexican in a hollywood movie from the all passably acted but again nothing thought the main villains were pretty well done and fairly well acted by the end of the movie you certainly knew who the good guys were and were not there was an emotional lift as the really bad ones got their just deserts very simplistic but then you were not expecting hamlet right the only thing i found really annoying was the constant cuts to vds daughter during the last fight bad not good passable\n",
      "Index 2: its a totally average film with a few semi alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films parts of the plot do not make sense and seem to be added in to use up tears in my eyes the end plot is that of a very basic type that does not leave the viewer guessing and any twists are obvious from the beginning the end scene with the flask backs do not make sense as they are added in and seem to have little relevance to the history of van dam character not really worth watching again bit disappointed in the end production even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n"
     ]
    }
   ],
   "source": [
    "# print the first three preprocessed text entries (test data) to verify the preprocessing step\n",
    "for index, value in testDataBasic.items():\n",
    "    print(f\"Index {index}: {value}\")\n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt4cK1IjJrY8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_downloader import download_and_save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_api import load_gensim_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_save_model('word2vec-google-news-300', 'word2vec_vector.kv')\n",
    "word2vec_wv = load_gensim_model('word2vec_vector.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_api import load_gensim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_save_model('glove-wiki-gigaword-300', 'glove_vector.kv')\n",
    "glove_wv = load_gensim_model('glove_vector.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tokenisation and Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nitialize and fit tokenizer on training data, then convert train and test data into integer sequences for model input\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainDataBasic)\n",
    "\n",
    "trainToken = tokenizer.texts_to_sequences(trainDataBasic)\n",
    "testToken = tokenizer.texts_to_sequences(testDataBasic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the 90th percentile of train sequence lengths to set max length, then pad train and test sequences to this length\n",
    "sequence_lengths = [len(x) for x in trainToken]\n",
    "percentile_90 = np.percentile(sequence_lengths, 90)\n",
    "max_length = int(percentile_90)\n",
    "\n",
    "trainPadded = pad_sequences(trainToken, maxlen=max_length, padding='post')\n",
    "testPadded = pad_sequences(testToken, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix by mapping each word in the tokenizer's word index to its corresponding word2vec vector\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_wv:\n",
    "        embedding_vector = word2vec_wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a non-trainable Embedding layer using the precomputed word2vec embedding matrix with specified dimensions\n",
    "embedding_word2vec = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an embedding matrix for the tokenizer's vocabulary using GloVe word vectors\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_wv:\n",
    "        embedding_vector = glove_wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a non-trainable GloVe-based Embedding layer for the model, using the pre-built embedding matrix\n",
    "embedding_glove = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom F1 Score metric class for TensorFlow, combining precision and recall calculations\n",
    "import tensorflow as tf\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a CNN model with Word2Vec embeddings, convolution, max pooling, dense layers, and compile with a custom F1 Score metric\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(embedding_word2vec)\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(10, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "622/623 [============================>.] - ETA: 0s - loss: 0.4797 - f1_score: 0.6122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momo\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2699: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 28s 44ms/step - loss: 0.4794 - f1_score: 0.6128 - val_loss: 0.5339 - val_f1_score: 0.9030\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 27s 43ms/step - loss: 0.3254 - f1_score: 0.8319 - val_loss: 0.4266 - val_f1_score: 0.9155\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 27s 43ms/step - loss: 0.2556 - f1_score: 0.8758 - val_loss: 0.5752 - val_f1_score: 0.8873\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 27s 43ms/step - loss: 0.1943 - f1_score: 0.9079 - val_loss: 0.3175 - val_f1_score: 0.9478\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 27s 43ms/step - loss: 0.1505 - f1_score: 0.9345 - val_loss: 0.4728 - val_f1_score: 0.9278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26b37218150>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the CNN model on padded training data and labels for 5 epochs with a 20% validation split\n",
    "\n",
    "model_cnn.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 13s 16ms/step - loss: 0.3241 - f1_score: 0.8839\n",
      "Test F1 Score CNN - Word2Vec: 0.8839179277420044\n"
     ]
    }
   ],
   "source": [
    "# evaluate the CNN model on test data and print the F1 Score for the Word2Vec-based model\n",
    "loss, f1_cnn_word2vec = model_cnn.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score CNN - Word2Vec: {f1_cnn_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(embedding_glove)\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(10, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 37s 57ms/step - loss: 0.5947 - f1_score: 0.0029 - val_loss: 0.7558 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 35s 56ms/step - loss: 0.5515 - f1_score: 0.0040 - val_loss: 0.7215 - val_f1_score: 0.0020\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 35s 56ms/step - loss: 0.5391 - f1_score: 0.5837 - val_loss: 0.7710 - val_f1_score: 0.8851\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 35s 56ms/step - loss: 0.5287 - f1_score: 0.6658 - val_loss: 0.7974 - val_f1_score: 0.8750\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 35s 56ms/step - loss: 0.5236 - f1_score: 0.6697 - val_loss: 0.7498 - val_f1_score: 0.9240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26b714f4ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 13s 17ms/step - loss: 0.4851 - f1_score: 0.8743\n",
      "Test F1 Score CNN - Glove: 0.8742826581001282\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_glove = model_cnn.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score CNN - Glove: {f1_cnn_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Neural Network - Long Short-Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, MaxPooling1D, LSTM\n",
    "\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(embedding_word2vec)\n",
    "model_cnn_lstm.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D(pool_size=4))\n",
    "model_cnn_lstm.add(LSTM(128))\n",
    "model_cnn_lstm.add(Dense(10, activation='relu'))\n",
    "model_cnn_lstm.add(Dropout(0.5))\n",
    "model_cnn_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn_lstm.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 52s 81ms/step - loss: 0.6660 - f1_score: 0.0209 - val_loss: 0.9629 - val_f1_score: 0.0905\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 51s 81ms/step - loss: 0.6552 - f1_score: 0.0666 - val_loss: 0.9766 - val_f1_score: 0.0084\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 52s 83ms/step - loss: 0.6578 - f1_score: 0.0334 - val_loss: 0.9717 - val_f1_score: 0.0363\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 52s 83ms/step - loss: 0.4594 - f1_score: 0.5734 - val_loss: 0.4370 - val_f1_score: 0.9148\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 51s 82ms/step - loss: 0.3308 - f1_score: 0.7605 - val_loss: 0.3381 - val_f1_score: 0.9390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26b717adb90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_lstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 20s 26ms/step - loss: 0.2862 - f1_score: 0.8867\n",
      "Test F1 Score - Word2Vec: 0.8866626024246216\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_lstm_word2vec = model_cnn_lstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - Word2Vec: {f1_cnn_lstm_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, MaxPooling1D\n",
    "\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(embedding_glove)\n",
    "model_cnn_lstm.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D(pool_size=4))\n",
    "model_cnn_lstm.add(LSTM(128))\n",
    "model_cnn_lstm.add(Dense(10, activation='relu'))\n",
    "\n",
    "model_cnn_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn_lstm.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 54s 83ms/step - loss: 0.6586 - f1_score: 0.1237 - val_loss: 0.9981 - val_f1_score: 0.0397\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 51s 82ms/step - loss: 0.6429 - f1_score: 0.2483 - val_loss: 0.7684 - val_f1_score: 0.1133\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 50s 81ms/step - loss: 0.4996 - f1_score: 0.5873 - val_loss: 0.5922 - val_f1_score: 0.8005\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 50s 81ms/step - loss: 0.2957 - f1_score: 0.8315 - val_loss: 0.7572 - val_f1_score: 0.8121\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 50s 81ms/step - loss: 0.2341 - f1_score: 0.8768 - val_loss: 0.4982 - val_f1_score: 0.8799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26ca3308e50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_lstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 19s 25ms/step - loss: 0.3140 - f1_score: 0.8586\n",
      "Test F1 Score - Glove: 0.8586170673370361\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_lstm_glove = model_cnn_lstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - Glove: {f1_cnn_lstm_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional Long Short-Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(embedding_word2vec)\n",
    "model_bilstm.add(Bidirectional(LSTM(128)))\n",
    "model_bilstm.add(Dense(10, activation='relu'))\n",
    "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', \n",
    "                     loss='binary_crossentropy', \n",
    "                     metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 290s 461ms/step - loss: 0.5687 - f1_score: 0.5340 - val_loss: 1.4011 - val_f1_score: 0.6574\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 262s 419ms/step - loss: 0.5728 - f1_score: 0.5607 - val_loss: 0.8160 - val_f1_score: 0.6090\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 243s 390ms/step - loss: 0.5859 - f1_score: 0.4663 - val_loss: 0.3912 - val_f1_score: 0.9181\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 244s 392ms/step - loss: 0.3332 - f1_score: 0.8150 - val_loss: 0.4023 - val_f1_score: 0.8990\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 244s 392ms/step - loss: 0.2882 - f1_score: 0.8432 - val_loss: 0.4222 - val_f1_score: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26ca6a83990>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 90s 116ms/step - loss: 0.3054 - f1_score: 0.8669\n",
      "Test F1 Score - BiLSTM Word2Vec: 0.8668519258499146\n"
     ]
    }
   ],
   "source": [
    "loss, f1_bilstm_word2vec = model_bilstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - BiLSTM Word2Vec: {f1_bilstm_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(embedding_glove)\n",
    "model_bilstm.add(Bidirectional(LSTM(128)))\n",
    "model_bilstm.add(Dense(10, activation='relu'))\n",
    "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', \n",
    "                     loss='binary_crossentropy', \n",
    "                     metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 318s 507ms/step - loss: 0.5738 - f1_score: 0.4994 - val_loss: 0.6707 - val_f1_score: 0.8236\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 297s 476ms/step - loss: 0.3466 - f1_score: 0.8029 - val_loss: 0.7687 - val_f1_score: 0.7521\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 310s 498ms/step - loss: 0.2849 - f1_score: 0.8434 - val_loss: 0.5548 - val_f1_score: 0.8809\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 323s 518ms/step - loss: 0.2490 - f1_score: 0.8626 - val_loss: 0.3897 - val_f1_score: 0.9085\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 320s 514ms/step - loss: 0.2081 - f1_score: 0.8926 - val_loss: 0.3151 - val_f1_score: 0.9357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26c7c607f90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 98s 126ms/step - loss: 0.2910 - f1_score: 0.8871\n",
      "Test F1 Score - BiLSTM Word2Vec: 0.8870954513549805\n"
     ]
    }
   ],
   "source": [
    "loss, f1_bilstm_glove = model_bilstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - BiLSTM Word2Vec: {f1_bilstm_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_results import generate_dl_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "kYpkjiSAVj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Embedding  F1-score\n",
      "0       CNN  Word2Vec  0.883918\n",
      "1       CNN     GloVe  0.874283\n",
      "2    BILSTM  Word2Vec  0.866852\n",
      "3    BILSTM     GloVe  0.887095\n",
      "4  CNN-LSTM  Word2Vec  0.886663\n",
      "5  CNN-LSTM     GloVe  0.858617\n"
     ]
    }
   ],
   "source": [
    "resultsDF = generate_dl_results_df(f1_cnn_word2vec, f1_cnn_glove, f1_bilstm_word2vec, f1_bilstm_glove, f1_cnn_lstm_word2vec, f1_cnn_lstm_glove)\n",
    "print(resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "kYpkjiSAVj0M"
   },
   "outputs": [],
   "source": [
    "resultsDF.to_csv(\"final_dl_models_basic_prep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuvptv9SVjrL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOL03ukDTATxLkpbBq0g8Ag",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1ZVDd_9O8eddBkUyyM9Vhr5rHvhTnLBol",
   "provenance": [
    {
     "file_id": "1ZVDd_9O8eddBkUyyM9Vhr5rHvhTnLBol",
     "timestamp": 1705376268080
    },
    {
     "file_id": "1hujpqpuuR_ekIzBjWsOOvxY0kmOzW50F",
     "timestamp": 1705255316719
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
