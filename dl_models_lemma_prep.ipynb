{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgD_3FJwaES2"
   },
   "source": [
    "# **Sentiment Classification - Deep Learning - Lemma Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLJhqTgS68An"
   },
   "source": [
    "# **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFnczcAc7Mav"
   },
   "source": [
    "**Install Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KSxo3JxS7Hxd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets -q\n",
    "!pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-0Iv3Os7QDx"
   },
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y0ne7wIg7UU7"
   },
   "outputs": [],
   "source": [
    "from imdb_data_loader import load_and_prepare_imdb_dataset\n",
    "\n",
    "trainData, testData = load_and_prepare_imdb_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTGBM1n97W6M"
   },
   "source": [
    "# **Dataset Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_8fuEvh7Zdq"
   },
   "source": [
    "**Data Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IxYSlnBL7dGO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vVbKoC4i7ndU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rjHEXnao7g0N"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First off let me say, If you haven't enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I love sci-fi and am willing to put up with a ...      0\n",
       "1  Worth the entertainment value of a rental, esp...      0\n",
       "2  its a totally average film with a few semi-alr...      0\n",
       "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
       "4  First off let me say, If you haven't enjoyed a...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "53ZeXaAK7lF1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojlNeIIlKJwH"
   },
   "source": [
    "**Remove Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DXi_uGDuKKlC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataDuplicates = trainData['text'].duplicated().sum()\n",
    "\n",
    "trainDataDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "64jpgJelKM9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24904, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noTrainDataDuplicates = trainData.drop_duplicates(subset='text')\n",
    "\n",
    "noTrainDataDuplicatesShape = noTrainDataDuplicates.shape\n",
    "\n",
    "noTrainDataDuplicatesShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DPVb3LSWKewZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataDuplicates = testData['text'].duplicated().sum()\n",
    "\n",
    "testDataDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u2C7uaJ4KfXL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24801, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noTestDataDuplicates = testData.drop_duplicates(subset='text')\n",
    "\n",
    "noTestDataDuplicatesShape = noTestDataDuplicates.shape\n",
    "\n",
    "noTestDataDuplicatesShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wwJa1nfKlqB"
   },
   "source": [
    "# **Lemma Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_oN-5HqOR3KV"
   },
   "outputs": [],
   "source": [
    "from lemma_preprocessing import preprocess_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oRQihYRZKoEN"
   },
   "outputs": [],
   "source": [
    "rawtrainData = noTrainDataDuplicates.copy()\n",
    "rawtestData = noTestDataDuplicates.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dtZ6gWLzKqTq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\research-dissertation\\codes\\lemma_preprocessing.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "trainDataLemma = rawtrainData['text'].apply(preprocess_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b5sOD8xuKuxk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: I rent I be curious yellow from my video store because of all the controversy that surround it when it be first release in 1967 I also hear that at first it be seize by you.s customs if it ever try to enter this country therefore be a fan of film consider controversial I really have to see this for myself.the plot be center around a young swedish drama student name lena who want to learn everything she can about life in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states in between ask politician and ordinary denizen of stockholm about their opinion on politic she have sex with her drama teacher classmate and marry men.what kill I about I be curious yellow be that 40 year ago this be consider pornographic really the sex and nudity scene be few and far between even then it have it be not shoot like some cheaply make porno while my countryman mind find it shocking in reality sex and nudity be a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford have sex scene in his films.i do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america I be curious yellow be a good film for anyone want to study the meat and potato no pun intend of swedish cinema but really this film do not have much of a plot\n",
      "Index 1: I be curious yellow be a risible and pretentious steaming pile it do not matter what one 's political view be because this film can hardly be take seriously on any level as for the claim that frontal male nudity be an automatic nc-17 that be not true I have see r rate film with male nudity grant they only offer some fleeting view but where be the r rate film with gape vulvas and flap labia nowhere because they do not exist the same go for those crappy cable show schlong swinge in the breeze but not a clitoris in sight and those pretentious indie movie like the brown bunny in which we be treat to the site of vincent gallo 's throbbing johnson but not a trace of pink visible on chloe sevigny before cry or imply double standard in matter of nudity the mentally obtuse should take into account one unavoidably obvious anatomical difference between man and woman there be no genital on display when actress appear nude and the same can not be say for a man in fact you generally will not see female genital in an american film in anything short of porn or explicit erotica this allege double standard be less a double standard than an admittedly depressing ability to come to term culturally with the inside of woman 's body\n",
      "Index 2: if only to avoid make this type of film in the future this film be interesting as an experiment but tell no cogent story.one might feel virtuous for sit thru it because it touch on so many important issue but it do so without any discernable motive the viewer come away with no new perspective unless one come up with one while one 's mind wander as it will invariably do during this pointless film).one might well spend one 's tear in my eye stare out a window at a tree grow\n"
     ]
    }
   ],
   "source": [
    "for index, value in trainDataLemma.items():\n",
    "    print(f\"Index {index}: {value}\")\n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jni2W2NjKwwB"
   },
   "outputs": [],
   "source": [
    "testDataLemma = rawtestData['text'].apply(preprocess_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IfkXIyy1Kxjh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: I love sci fi and be willing to put up with a lot sci fi movie tv be usually underfunded under appreciated and misunderstood I try to like this I really do but it be to good tv sci fi as babylon 5 be to star trek the original silly prosthetic cheap cardboard set stilte dialogue cg that do not match the background and painfully one dimensional character can not be overcome with a sci fi setting I be sure there be those of you out there who think babylon 5 be good sci fi tv it have it be not it have it be clich√©d and uninspire while we viewer might like emotion and character development sci fi be a genre that do not take itself seriously cf star trek it may treat important issue yet not as a serious philosophy it have it be really difficult to care about the character here as they be not simply foolish just miss a spark of life their action and reaction be wooden and predictable often painful to watch the maker of earth know it have it be rubbish as they have to always say gene roddenberry 's earth otherwise people would not continue watch roddenberry 's ashe must be turn in their orbit as this dull cheap poorly edit watch it without advert break really bring this home trudge trabant of a show lumber into space spoiler so kill off a main character and then bring he back as another actor jeeez dallas all over again\n",
      "Index 1: worth the entertainment value of a rental especially if you like action movie this one feature the usual car chase fight with the great van damme kick style shoot battle with the 40 shell load shotgun and even terrorist style bomb all of this be entertaining and competently handle but there be nothing that really blow you away if you have see your share before.the plot be make interesting by the inclusion of a rabbit which be clever but hardly profound many of the character be heavily stereotype the angry veteran the terrified illegal alien the crooked cop the indifferent fed the bitchy tough lady station head the crooked politician the fat federale who look like he be typecast as the mexican in a hollywood movie from the 1940 all passably act but again nothing special.i think the main villain be pretty well do and fairly well act by the end of the movie you certainly know who the good guy be and be not there be an emotional lift as the really bad one get their just desert very simplistic but then you be not expect hamlet right the only thing I find really annoying be the constant cut to vds daughter during the last fight scene.not bad not good passable 4\n",
      "Index 2: its a totally average film with a few semi alright action sequence that make the plot seem a little well and remind the viewer of the classic van dam film part of the plot do not make sense and seem to be add in to use up tear in my eye the end plot be that of a very basic type that do not leave the viewer guessing and any twist be obvious from the beginning the end scene with the flask back do not make sense as they be add in and seem to have little relevance to the history of van dam 's character not really worth watch again bit disappoint in the end production even though it be apparent it be shoot on a low budget certain shot and section in the film be of poor directed quality\n"
     ]
    }
   ],
   "source": [
    "for index, value in testDataLemma.items():\n",
    "    print(f\"Index {index}: {value}\")\n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt4cK1IjJrY8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_downloader import download_and_save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_api import load_gensim_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download_and_save_model('word2vec-google-news-300', 'word2vec_vector.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_wv = load_gensim_model('word2vec_vector.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim_model_api import load_gensim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_save_model('glove-wiki-gigaword-300', 'glove_vector.kv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wv = load_gensim_model('glove_vector.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tokenisation and Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainDataLemma)\n",
    "\n",
    "trainToken = tokenizer.texts_to_sequences(trainDataLemma)\n",
    "testToken = tokenizer.texts_to_sequences(testDataLemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(x) for x in trainToken]\n",
    "percentile_90 = np.percentile(sequence_lengths, 90)\n",
    "max_length = int(percentile_90)\n",
    "\n",
    "trainPadded = pad_sequences(trainToken, maxlen=max_length, padding='post')\n",
    "testPadded = pad_sequences(testToken, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_wv:\n",
    "        embedding_vector = word2vec_wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_word2vec = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_wv:\n",
    "        embedding_vector = glove_wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_glove = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(embedding_word2vec)\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(10, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 27s 42ms/step - loss: 0.5014 - f1_score: 0.5918 - val_loss: 0.4877 - val_f1_score: 0.8941\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.3954 - f1_score: 0.7164 - val_loss: 0.6508 - val_f1_score: 0.8251\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.3398 - f1_score: 0.7532 - val_loss: 0.4613 - val_f1_score: 0.8887\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.2932 - f1_score: 0.7840 - val_loss: 0.2581 - val_f1_score: 0.9459\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.2598 - f1_score: 0.8143 - val_loss: 0.2979 - val_f1_score: 0.9352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ee9050d710>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 11s 14ms/step - loss: 0.2765 - f1_score: 0.8838\n",
      "Test F1 Score CNN - Word2Vec: 0.8838421106338501\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_word2vec = model_cnn.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score CNN - Word2Vec: {f1_cnn_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(embedding_glove)\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(10, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 27s 42ms/step - loss: 0.5270 - f1_score: 0.4510 - val_loss: 0.6129 - val_f1_score: 0.8621\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 25s 41ms/step - loss: 0.4248 - f1_score: 0.5667 - val_loss: 0.5092 - val_f1_score: 0.9001\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.3727 - f1_score: 0.6904 - val_loss: 0.6036 - val_f1_score: 0.8463\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 25s 41ms/step - loss: 0.3104 - f1_score: 0.7924 - val_loss: 0.4062 - val_f1_score: 0.9053\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 26s 41ms/step - loss: 0.2456 - f1_score: 0.8617 - val_loss: 0.7344 - val_f1_score: 0.8206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ee9e019b10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 11s 14ms/step - loss: 0.4012 - f1_score: 0.8102\n",
      "Test F1 Score CNN - Glove: 0.8101916909217834\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_glove = model_cnn.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score CNN - Glove: {f1_cnn_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Neural Network - Long Short-Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, MaxPooling1D, LSTM\n",
    "\n",
    "# Assuming 'embedding_layer' is your pre-initialized Embedding layer\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(embedding_word2vec)\n",
    "\n",
    "# Convolutional Layer\n",
    "model_cnn_lstm.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "# LSTM Layer\n",
    "model_cnn_lstm.add(LSTM(128))\n",
    "\n",
    "# Fully Connected Layers\n",
    "model_cnn_lstm.add(Dense(10, activation='relu'))\n",
    "\n",
    "model_cnn_lstm.add(Dense(1, activation='sigmoid'))  # Use 'softmax' if you have multiple classes\n",
    "\n",
    "model_cnn_lstm.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 71s 114ms/step - loss: 0.2251 - f1_score: 0.8831 - val_loss: 0.5101 - val_f1_score: 0.8637\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 71s 114ms/step - loss: 0.1858 - f1_score: 0.9054 - val_loss: 0.3543 - val_f1_score: 0.9189\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 71s 114ms/step - loss: 0.1427 - f1_score: 0.9321 - val_loss: 0.5326 - val_f1_score: 0.8879\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 73s 117ms/step - loss: 0.1083 - f1_score: 0.9532 - val_loss: 0.3045 - val_f1_score: 0.9430\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 70s 113ms/step - loss: 0.0698 - f1_score: 0.9728 - val_loss: 0.4281 - val_f1_score: 0.9221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eeaaac05d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_lstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 29s 38ms/step - loss: 0.3406 - f1_score: 0.8894\n",
      "Test F1 Score - Word2Vec: 0.889437198638916\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_lstm_word2vec = model_cnn_lstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - Word2Vec: {f1_cnn_lstm_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, MaxPooling1D, LSTM\n",
    "\n",
    "# Assuming 'embedding_layer' is your pre-initialized Embedding layer\n",
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(embedding_glove)\n",
    "\n",
    "# Convolutional Layer\n",
    "model_cnn_lstm.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model_cnn_lstm.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "# LSTM Layer\n",
    "model_cnn_lstm.add(LSTM(128))\n",
    "\n",
    "# Fully Connected Layers\n",
    "model_cnn_lstm.add(Dense(10, activation='relu'))\n",
    "\n",
    "model_cnn_lstm.add(Dense(1, activation='sigmoid'))  # Use 'softmax' if you have multiple classes\n",
    "\n",
    "model_cnn_lstm.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[F1Score()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 76s 114ms/step - loss: 0.6634 - f1_score: 0.0000e+00 - val_loss: 0.9305 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 70s 112ms/step - loss: 0.6597 - f1_score: 0.0564 - val_loss: 0.9593 - val_f1_score: 0.0547\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 70s 113ms/step - loss: 0.6484 - f1_score: 0.1936 - val_loss: 0.9330 - val_f1_score: 0.1456\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 63s 102ms/step - loss: 0.6260 - f1_score: 0.2699 - val_loss: 1.3045 - val_f1_score: 0.4536\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 56s 90ms/step - loss: 0.3959 - f1_score: 0.7677 - val_loss: 0.3531 - val_f1_score: 0.9223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eecb0ab6d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_lstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 25s 33ms/step - loss: 0.3503 - f1_score: 0.8511\n",
      "Test F1 Score - Glove: 0.8511236906051636\n"
     ]
    }
   ],
   "source": [
    "loss, f1_cnn_lstm_glove = model_cnn_lstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - Glove: {f1_cnn_lstm_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional Long Short-Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(embedding_word2vec)\n",
    "model_bilstm.add(Bidirectional(LSTM(128)))\n",
    "model_bilstm.add(Dense(10, activation='relu'))\n",
    "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', \n",
    "                     loss='binary_crossentropy', \n",
    "                     metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 410s 648ms/step - loss: 0.6160 - f1_score: 0.3784 - val_loss: 0.8103 - val_f1_score: 0.6708\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 346s 555ms/step - loss: 0.5519 - f1_score: 0.5958 - val_loss: 0.8675 - val_f1_score: 0.6918\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 305s 489ms/step - loss: 0.4688 - f1_score: 0.7032 - val_loss: 0.3836 - val_f1_score: 0.9259\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 275s 442ms/step - loss: 0.3220 - f1_score: 0.8201 - val_loss: 0.3972 - val_f1_score: 0.9082\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 262s 420ms/step - loss: 0.2854 - f1_score: 0.8428 - val_loss: 0.4714 - val_f1_score: 0.8784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eeaab06d50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 140s 181ms/step - loss: 0.3325 - f1_score: 0.8509\n",
      "Test F1 Score - BiLSTM Word2Vec: 0.8508748412132263\n"
     ]
    }
   ],
   "source": [
    "loss, f1_bilstm_word2vec = model_bilstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - BiLSTM Word2Vec: {f1_bilstm_word2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(embedding_glove)\n",
    "model_bilstm.add(Bidirectional(LSTM(128)))\n",
    "model_bilstm.add(Dense(10, activation='relu'))\n",
    "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', \n",
    "                     loss='binary_crossentropy', \n",
    "                     metrics=[F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "623/623 [==============================] - 249s 395ms/step - loss: 0.5028 - f1_score: 0.5963 - val_loss: 0.5963 - val_f1_score: 0.8374\n",
      "Epoch 2/5\n",
      "623/623 [==============================] - 249s 399ms/step - loss: 0.3181 - f1_score: 0.8195 - val_loss: 0.2730 - val_f1_score: 0.9415\n",
      "Epoch 3/5\n",
      "623/623 [==============================] - 248s 398ms/step - loss: 0.2737 - f1_score: 0.8513 - val_loss: 0.3813 - val_f1_score: 0.9054\n",
      "Epoch 4/5\n",
      "623/623 [==============================] - 249s 399ms/step - loss: 0.2316 - f1_score: 0.8795 - val_loss: 0.5133 - val_f1_score: 0.8814\n",
      "Epoch 5/5\n",
      "623/623 [==============================] - 252s 404ms/step - loss: 0.1911 - f1_score: 0.9024 - val_loss: 0.4999 - val_f1_score: 0.8929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eec4c1b710>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm.fit(trainPadded, rawtrainData['label'], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 128s 165ms/step - loss: 0.3136 - f1_score: 0.8701\n",
      "Test F1 Score - BiLSTM Glove: 0.870068371295929\n"
     ]
    }
   ],
   "source": [
    "loss, f1_bilstm_glove = model_bilstm.evaluate(testPadded, rawtestData['label'])\n",
    "print(f\"Test F1 Score - BiLSTM Glove: {f1_bilstm_glove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_results import generate_dl_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "kYpkjiSAVj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Embedding  F1-score\n",
      "0       CNN  Word2Vec  0.883842\n",
      "1       CNN     GloVe  0.810192\n",
      "2    BILSTM  Word2Vec  0.850875\n",
      "3    BILSTM     GloVe  0.870068\n",
      "4  CNN-LSTM  Word2Vec  0.889437\n",
      "5  CNN-LSTM     GloVe  0.851124\n"
     ]
    }
   ],
   "source": [
    "resultsDF = generate_dl_results_df(f1_cnn_word2vec, f1_cnn_glove, f1_bilstm_word2vec, f1_bilstm_glove, f1_cnn_lstm_word2vec, f1_cnn_lstm_glove)\n",
    "print(resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "kYpkjiSAVj0M"
   },
   "outputs": [],
   "source": [
    "resultsDF.to_csv(\"final_dl_models_lemma_prep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuvptv9SVjrL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOL03ukDTATxLkpbBq0g8Ag",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1ZVDd_9O8eddBkUyyM9Vhr5rHvhTnLBol",
   "provenance": [
    {
     "file_id": "1ZVDd_9O8eddBkUyyM9Vhr5rHvhTnLBol",
     "timestamp": 1705376268080
    },
    {
     "file_id": "1hujpqpuuR_ekIzBjWsOOvxY0kmOzW50F",
     "timestamp": 1705255316719
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
